{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv9Nytklkhyd"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introducing gated convolutions\n",
    "In the first experiment with a basic convolutional neural network (CNN), we observed that the model produced various types of visual artifacts, from discolouration to patterns that resembled the shape of the patch, and we hypothesized that it arose from convolutions operating on the hole values (set to 0, by default).\n",
    "\n",
    "Following the work of [Liu et al. (2018)](https://arxiv.org/pdf/1804.07723.pdf)and [Yu et. al (2018)](https://arxiv.org/abs/1806.03589), we implement gated convolutions as a simple yet flexible way to ensure that \"invalid\" pixels are not used in the image generation process in a learnable fashion.\n",
    "\n",
    "As a means of providing more information to the model, we also provide the mask as an input channel.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Modified U-Net\n",
    "\n",
    "### 2.1 Choice of U-Net architecture\n",
    "\n",
    "In our earlier experiment with a CNN, we noted that skip connections would likely help with faster training. We also noted that maintaining activation maps of equal dimensions at all layers was computationally expensive. To that end, a model design with skip connections, a downsampling then upsampling operation with the ability to reconstruct pixel-level details would be ideal. The U-Net architecture proposed by [Ronneberger et al. (2015)](https://arxiv.org/abs/1505.04597) suits this.\n",
    "\n",
    "### 2.2 Introducing residual skips\n",
    "\n",
    "As argued by [He et al. (2015)](https://arxiv.org/abs/1512.03385), residual skips allow for the practical training of deeper neural networks with higher representational power, as the improved gradient flow alleviates the vanishing gradient problem and residual skips tend toward the identity function, rather than the zero function. \n",
    "\n",
    "To increase the receptive field of each masked pixel in the output, we need to deepen the network, and so introduce residual skips as a practical matter. \n",
    "\n",
    "### 2.3 Downsampling by convolutions, rather than pooling\n",
    "\n",
    "In the original paper, the U-Net model was designed for image segmentation and uses max-pooling for downsampling. Owing to the translation and (somewhat) rotation invariance of max pooling, this operator works well for segmentation. But the loss of localization information likely leads to lossy downsampling in the application to image generation for inpainting, so instead, we carry out downsampling by convolutions with larger strides.\n",
    "\n",
    "So, we use a U-Net like architecture, with the added difference of residual skips at every block to improve gradient flow, as well as downsampling using convolutions, rather than pooling to avoid lossy downsampling (since our goal is to reconstruct the image).\n",
    "\n",
    "### 2.4. Upsampling by convolutions and pixel shuffling\n",
    "\n",
    "There is [evidence](https://distill.pub/2016/deconv-checkerboard/) to suggest that the transposed convolution operation as a manner of learnable upsampling tends to generate checkboard-like artefacts, due to overlapping points in the application of the convolution filter at the low-resolution feature maps. \n",
    "\n",
    "An alternative, thus, is to use nearest-neighbour interpolation paired with convolutions to upsample images. However, [Shi et al. (2017)](https://arxiv.org/ftp/arxiv/papers/1609/1609.07009.pdf) argue that carrying out upsampling by what they call \"pixel shuffling\" has higher representational power for the same computational complexity. The operation they propose works by using convolutions to generate *n x r<sup>2</sup> x h x w* feature maps from an input with *n* feature maps, and then \"shuffling\" these feature maps together to get an output of *n x (hr) x (wr)*. The intuition behind its greater representational power given the same time complexity, is that the convolutions are operating in the lower-resolution spce, rather than the higher-resolution space after upsampling - so more parameters can be learnt. \n",
    "\n",
    "---\n",
    "\n",
    "## 3. A segmented loss function\n",
    "\n",
    "In the basic CNN model, we trained the model with L1 loss, with the intuition that it would aid in high-accuracy pixel contruction. \n",
    "\n",
    "We observed empirically that the masked areas were much smaller than the overall image, and the error signal would probably not encourage the careful reconstruction of the masked regions. So instead, we can separate these two losess - one for the masked region, one for the unmasked region, and then weight their losses as a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIEkShUFq9FA"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek1yIysk0dFf"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tg6kdj7imCuM",
    "outputId": "a21d6c6e-1e58-4bbe-e0fe-4b58c5fb3cc6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# images\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "\n",
    "# metrics\n",
    "!pip install torchmetrics\n",
    "!pip install lpips\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gkmmp4cp0e6O"
   },
   "source": [
    "## Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLGwoLSyq_MF",
    "outputId": "44749dd1-6da3-43bf-b87f-ae65ad71a53b"
   },
   "outputs": [],
   "source": [
    "# Mount data from drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# make a local data directory\n",
    "!rm -r /content/data\n",
    "!mkdir /content/data\n",
    "!mkdir /content/data/images\n",
    "!mkdir /content/data/preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccfcFn-75K1K",
    "outputId": "56a34333-919e-4d35-8c14-eb477c4ecb7a"
   },
   "outputs": [],
   "source": [
    "dataset_ids = {\n",
    "    'frogs': '1hq3Bjd1tapiVt_qmqb6uP8rA5ECutXCE',\n",
    "    'frog_indices' : '1Td2csrwues_ZSqDCXSIBLLx-Ab7Z0KIJ'\n",
    "}\n",
    "\n",
    "# get images\n",
    "file_id = dataset_ids['frogs']\n",
    "file_name = '/content/data/frogs_15k_renamed.zip'\n",
    "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={file_id}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={file_id}\" -O {file_name} && rm -rf /tmp/cookies.txt\n",
    "\n",
    "# unzip file\n",
    "!unzip /content/data/frogs_15k_renamed.zip -d /content/data/images\n",
    "\n",
    "# get index file\n",
    "file_id = dataset_ids['frog_indices']\n",
    "file_name = '/content/data/frogs_wanted_indices_url.txt'\n",
    "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={file_id}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={file_id}\" -O {file_name} && rm -rf /tmp/cookies.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDxlOnFF0h9r"
   },
   "source": [
    "## Import custom methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yk-htrdf570l",
    "outputId": "747f4cef-38bb-460c-85ca-af102652c3be"
   },
   "outputs": [],
   "source": [
    "# dataset and training functions\n",
    "%cd /content/drive/MyDrive/CS4243\\ Project\n",
    "from dataset_util.AnimalDataset import AnimalDataset\n",
    "from train_util.train_util import sample_batch, visualize_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIHoVHlLkhGG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAEU9gtRrStE"
   },
   "source": [
    "## Define model\n",
    "Here, we define three main classes to encapsulate the logoic - a `GatedConv2d` module as a fundemental module, a `GatedConv2dBlock` class that encapsulates batch normalization and activation, and then then main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Jq6RUUsrSC2"
   },
   "outputs": [],
   "source": [
    "class GatedConv2d(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a gated convolution. It works by applying a convolution filter \n",
    "    to the input feature tensor, then using the sigmoid function to map each score to a \n",
    "    pixel validity weight. The weights are then multiplied to the activation map from a \n",
    "    separate convolution filter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(GatedConv2d, self).__init__()\n",
    "        self.image_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.gate_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        mask = self.sigmoid(self.gate_conv(input_tensor))\n",
    "        x = self.image_conv(input_tensor)\n",
    "        x = torch.mul(x, mask) # apply mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GatedConv2dBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the following:\n",
    "    GatedConv2d -> BN -> activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, activation = nn.ReLU, p = 0.2):\n",
    "        super(GatedConv2dBlock, self).__init__()\n",
    "        self.conv = GatedConv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features = out_channels)\n",
    "        self.activation = activation()\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        x = self.conv(input_tensor)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GatedConv2DPixelShuffleUpsample(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates a convolution layer using GatedConv\n",
    "    and PixelShuffling to upsample. Returns same number of channels, with\n",
    "    output spatial dimensions scaled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, upscale_factor = 2):\n",
    "\n",
    "        super(GatedConv2DPixelShuffleUpsample, self).__init__()\n",
    "        self.conv = GatedConv2dBlock(in_channels, scale_factor ** 2 * in_channels, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.upsample = nn.PixelShuffle(upscale_factor = upscale_factor)\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        x = self.conv(input_tensor)\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class GatedConvUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates a UNet-like model with gated convolution blocks and residual skips.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 blocks_per_layer = 2,\n",
    "                 in_channels = 4, \n",
    "                 hidden_channels = 32, \n",
    "                 kernel_size = 3, \n",
    "                 stride = 1, \n",
    "                 padding = 'same',\n",
    "                 activation = nn.ReLU):\n",
    "\n",
    "        super(GatedConvUNet, self).__init__()\n",
    "\n",
    "        # ===== DOWNSAMPLING =====\n",
    "        \n",
    "        # first convolution\n",
    "        self.first = GatedConv2dBlock(in_channels, hidden_channels, kernel_size, stride, padding, activation)\n",
    "\n",
    "        # convolutions\n",
    "        self.down_1 = nn.ModuleList(\n",
    "            [GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])\n",
    "\n",
    "        self.down_2 = nn.ModuleList(\n",
    "            [GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])        \n",
    "\n",
    "        self.down_3 = nn.ModuleList(\n",
    "            [GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])  \n",
    "        \n",
    "        # downsampling by convolution\n",
    "        self.downtransform_1 = GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size = 4, stride = 2, padding = 1, activation = activation)\n",
    "        self.downtransform_2 = GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size = 4, stride = 2, padding = 1, activation = activation)\n",
    "        self.downtransform_3 = GatedConv2dBlock(hidden_channels, 2 * hidden_channels, kernel_size = 4, stride = 2, padding = 1, activation = activation)\n",
    "\n",
    "        # ===== MIDDLE =====\n",
    "        \n",
    "        self.middle = nn.ModuleList(\n",
    "            [GatedConv2dBlock(2 * hidden_channels, 2 * hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])  \n",
    "        \n",
    "\n",
    "        # ===== UPSAMPLING =====\n",
    "        # upsample and convolution, always doubling\n",
    "        self.upsample_1 = GatedConv2DPixelShuffleUpsample(in_channels = 2 * hidden_channels, upscale_factor = 2)\n",
    "        self.uptransform_1 = GatedConv2dBlock(2 * hidden_channels, hidden_channels, kernel_size, stride, padding, activation)\n",
    "        \n",
    "        self.upsample_2 = GatedConv2DPixelShuffleUpsample(in_channels = hidden_channels, upscale_factor = 2)\n",
    "        self.uptransform_2 = GatedConv2dBlock(2 * hidden_channels, hidden_channels, kernel_size, stride, padding, activation)\n",
    "\n",
    "        self.upsample_3 = GatedConv2DPixelShuffleUpsample(in_channels = hidden_channels, upscale_factor = 2)\n",
    "        self.uptransform_3 = GatedConv2dBlock(2 * hidden_channels, hidden_channels, kernel_size, stride, padding, activation)\n",
    "\n",
    "        # convolutions\n",
    "        self.up_1 = nn.ModuleList(\n",
    "            [GatedConv2dBlock(2 * hidden_channels, 2 * hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])\n",
    "        \n",
    "        self.up_2 = nn.ModuleList(\n",
    "            [GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])\n",
    "        \n",
    "        self.up_3 = nn.ModuleList(\n",
    "            [GatedConv2dBlock(hidden_channels, hidden_channels, kernel_size, stride, padding, activation) \n",
    "             for i in range(blocks_per_layer)])\n",
    "\n",
    "        # final scores\n",
    "        self.final = nn.Conv2d(hidden_channels, 3, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        # ===== DOWNSAMPLING =====\n",
    "\n",
    "        # first layer\n",
    "        x1 = self.first(input_tensor)\n",
    "        for l in self.down_1:\n",
    "            x1 = x1 + l(x1) # residual skips\n",
    "        \n",
    "        # second layer \n",
    "        x2 = self.downtransform_1(x1)\n",
    "        for l in self.down_2:\n",
    "            x2 = x2 + l(x2)\n",
    "\n",
    "        # third layer \n",
    "        x3 = self.downtransform_2(x2)\n",
    "        for l in self.down_3:\n",
    "            x3 = x3 + l(x3)\n",
    "\n",
    "        # ===== BOTTOM ====\n",
    "        x = self.downtransform_3(x3)\n",
    "        for l in self.middle:\n",
    "            x = x + l(x)\n",
    "\n",
    "        # ===== UPSAMPLING =====\n",
    "\n",
    "        # first layer\n",
    "        x = self.upsample_1(x) # upsample\n",
    "        x = torch.concat([x, x3], dim = 1) # concat in channel dimension (second)\n",
    "        x = self.uptransform_1(x)\n",
    "        for l in self.up_1:\n",
    "            x = x + l(x)\n",
    "        \n",
    "        # second layer \n",
    "        x = self.upsample_2(x) # upsample\n",
    "        x = torch.concat([x, x2], dim = 1) # concat in channel dimension (second)\n",
    "        x = self.uptransform_2(x)\n",
    "        for l in self.up_2:\n",
    "            x = x + l(x)\n",
    "\n",
    "        # third layer\n",
    "        x = self.upsample_3(x) # upsample\n",
    "        x = torch.concat([x, x1], dim = 1) # concat in channel dimension (second)\n",
    "        x = self.uptransform_3(x)\n",
    "        for l in self.up_3:\n",
    "            x = x + l(x)\n",
    "\n",
    "        # ===== FINAL =====\n",
    "        x = self.final(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "    def summary(self, verbose = False):\n",
    "        count = 0\n",
    "        if verbose:\n",
    "            print(self)\n",
    "\n",
    "        for name, params in self.named_parameters():\n",
    "            num_params = params.flatten().size()[0]\n",
    "            count += num_params\n",
    "            if verbose:\n",
    "                print(f\"\\nlayer: {name}\")\n",
    "                print(f\"number of params: {num_params}\")\n",
    "                print(f\"params shape: {params.size()}\")\n",
    "\n",
    "        print(f\"model has {count/1e6} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN1aWwAirTol"
   },
   "source": [
    "## Initialize model\n",
    "Note that there is a significant difference in the depth (and thus, number of parameters) in the model. There are equivalently 17 layers in this model, relative to the 10 in the original CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9HDdG8KrXT2",
    "outputId": "975b9e54-658e-4d2b-f39c-af88c8d436fb"
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "blocks_per_layer = 1\n",
    "in_channels = 3\n",
    "hidden_channels = 32\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 'same'\n",
    "activation = nn.PReLU\n",
    "\n",
    "# create model\n",
    "model = GatedConvUNet(blocks_per_layer,\n",
    "                      in_channels, \n",
    "                      hidden_channels, \n",
    "                      kernel_size, \n",
    "                      stride, \n",
    "                      padding,\n",
    "                      activation)\n",
    "model.summary(verbose = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT5zY9vNrYG8"
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ7c6shC7gL_",
    "outputId": "e0cee5f9-59a4-4453-d6a8-a380c38e0328"
   },
   "outputs": [],
   "source": [
    "# go to directory\n",
    "%cd /content/drive/MyDrive/CS4243\\ Project\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-8GxXZOgrFl",
    "outputId": "29bcec51-9473-430e-d83a-f7a6813fbfc0"
   },
   "outputs": [],
   "source": [
    "# save path\n",
    "MODEL_NAME = \"GatedConvPixelShuffleUNet_3downup_1blocksperlayer_32hidden_relu\"\n",
    "SAVE_PATH = f\"{os.getcwd()}/Naaman/models/{MODEL_NAME}\"\n",
    "\n",
    "# save\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "# sanity check\n",
    "loaded = GatedConvUNet(blocks_per_layer,\n",
    "                       in_channels, \n",
    "                       hidden_channels, \n",
    "                       kernel_size, \n",
    "                       stride, \n",
    "                       padding,\n",
    "                       activation)\n",
    "\n",
    "loaded.load_state_dict(torch.load(SAVE_PATH))\n",
    "loaded.summary()\n",
    "del loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrDuLW2urZlN"
   },
   "source": [
    "# Training Details\n",
    "For training, we define all details here, and wrap it into two main dictionaries, `training_params` and `metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_segmented_loss(output, target, mask):\n",
    "    \"\"\"\n",
    "    This is a custom functional loss function that considers where the mask is, and\n",
    "    applies the loss function seperately. \n",
    "    \"\"\"\n",
    "    \n",
    "    MASKED_WEIGHT = 10\n",
    "    UNMASKED_WEIGHT = 1\n",
    "    LOSS = nn.functional.l1_loss\n",
    "\n",
    "    # weights by mask, unmasked as 1\n",
    "    masked_weights = (mask - 1) * -1 * MASKED_WEIGHT # invert the mask in a differentiable way\n",
    "    unmasked_weights = mask * UNMASKED_WEIGHT\n",
    "    \n",
    "    # weighted sum\n",
    "    loss_masked = LOSS(output, target, reduction = 'none')\n",
    "    loss_unmasked = LOSS(output, target, reduction = 'none')\n",
    "    loss = torch.mul(loss_masked, masked_weights) + torch.mul(loss_unmasked, unmasked_weights)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VElJr7yvrnvu"
   },
   "source": [
    "## Training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndGZyFYJrnRD"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = 'cuda'  \n",
    "model = model.to(device)\n",
    "\n",
    "# define training parameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer parameters\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
    "\n",
    "# loss function \n",
    "loss_function = mask_segmented_loss\n",
    "\n",
    "# wrap into dictionary\n",
    "training_params = {\n",
    "    \"num_epochs\" : num_epochs,\n",
    "    \"batch_size\" : batch_size, \n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer\" : optimizer,\n",
    "    \"scheduler\" : scheduler,\n",
    "    \"schedule_every\" : 1,\n",
    "    \"loss_function\" : loss_function,\n",
    "    \"save_path\" : SAVE_PATH\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtnlBdXjrqqj"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVM4T_Csrml2"
   },
   "outputs": [],
   "source": [
    "# define metrics\n",
    "metrics = {\n",
    "    \"Multiscale structural similarity index measure\" : torchmetrics.functional.multiscale_structural_similarity_index_measure,\n",
    "    \"Peak SnR\" : torchmetrics.functional.peak_signal_noise_ratio\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEPLOIhtrr0S"
   },
   "source": [
    "## Load Frogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFzmtDtortA_",
    "outputId": "732402b5-4d8b-4e9b-f0d2-eb23076899aa"
   },
   "outputs": [],
   "source": [
    "dataset = AnimalDataset(index_file_path = \"/content/data/frogs_wanted_indices_url.txt\",\n",
    "                        root_dir_path = \"/content/data/images/frogs_15k_renamed\",\n",
    "                        local_dir_path = \"/content/data/preprocessed\",\n",
    "                        file_prefix = \"frog_\",\n",
    "                        image_dimension = 256,\n",
    "                        concat_mask = True)\n",
    "\n",
    "dataset.initialize()\n",
    "\n",
    "# train-test split\n",
    "VALID_SIZE = 0.2\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "indices = torch.arange(len(dataset))\n",
    "train_indices, validation_indices = train_test_split(indices, test_size = VALID_SIZE)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "validation_dataset = Subset(dataset, validation_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744
    },
    "id": "cbNiSuc0l0nr",
    "outputId": "e9233e81-c12d-49e2-817d-6f2905d6a27c"
   },
   "outputs": [],
   "source": [
    "sample_batch(dataset, sample_size = 16)\n",
    "sample_batch(train_dataset, sample_size = 16)\n",
    "sample_batch(validation_dataset, sample_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cafLR4dBruM7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkpugxaamLaC"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbbf46Ylrvnc"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_dataloader, training_params : dict, metrics : dict):\n",
    "    \"\"\"\n",
    "    This method encapsulates the training of a given model for one epoch. \n",
    "    \n",
    "    @param model : nn.Module                        Model to be trained.\n",
    "    @param device : str                             Device to be trained on.\n",
    "    @param train_dataloader : nn.data.DataLoader    DataLoader object to load batches of data.\n",
    "    @param training_params : dict                   Dictionary object mapping names of \n",
    "                                                    training utilities to their respective objects.\n",
    "                                                    Required are \"batch_size\", \"loss_function\", \n",
    "                                                    and \"optimizer\". \n",
    "    @param metrics : dict                           Dictionary object mapping names of \n",
    "                                                    metrics to a functional method that \n",
    "                                                    would compute the metric value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== INITIALIZE =====\n",
    "    # constants\n",
    "    BATCH_SIZE = training_params[\"batch_size\"]\n",
    "    LOSS_FUNCTION = training_params[\"loss_function\"]\n",
    "    OPTIMIZER = training_params[\"optimizer\"]\n",
    "    BATCH_EVALUATE_EVERY = 1\n",
    "\n",
    "    # model to device\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # epoch metrics\n",
    "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
    "    running_results[\"loss\"] = 0.0\n",
    "\n",
    "    # ===== TRAIN EPOCH =====\n",
    "    num_batches = 0\n",
    "    for index, batch in enumerate(train_dataloader, 1):\n",
    "\n",
    "            # ===== INITIALIZE =====\n",
    "            num_batches += 1\n",
    "\n",
    "            # input and ground truth\n",
    "            input_batched = batch[\"image\"]\n",
    "            ground_truth_batched = batch[\"reconstructed\"]\n",
    "            mask_batched = batch[\"mask\"]\n",
    "\n",
    "            # sanity check\n",
    "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
    "\n",
    "            # move tensors to device\n",
    "            input_batched = input_batched.to(device)\n",
    "            ground_truth_batched = ground_truth_batched.to(device)\n",
    "            mask_batched = mask_batched.to(device)\n",
    "\n",
    "            # Set the gradients to zeros\n",
    "            OPTIMIZER.zero_grad()\n",
    "\n",
    "            # ===== FORWARD PASS =====\n",
    "            # reshape to channel first\n",
    "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
    "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
    "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
    "\n",
    "            # forward pass\n",
    "            input_batched.requires_grad_()\n",
    "            output_batched = model(input_batched)\n",
    "\n",
    "            # ===== BACKPROP =====\n",
    "\n",
    "            loss = LOSS_FUNCTION(output_batched, ground_truth_batched, mask_batched)\n",
    "            loss.backward()\n",
    "            OPTIMIZER.step()\n",
    "\n",
    "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
    "            # for each key, compute, add item to results dictionary\n",
    "            running_results[\"loss\"] += loss.detach().item()\n",
    "            for key, func in metrics.items():\n",
    "                running_results[key] += func(output_batched, ground_truth_batched).detach().item()\n",
    "\n",
    "            # ===== HOUSEKEEPING =====\n",
    "            del loss\n",
    "            del input_batched\n",
    "            del output_batched\n",
    "\n",
    "            # print results every some batches\n",
    "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
    "\n",
    "                args = \"\"\n",
    "                for key, val in running_results.items():\n",
    "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
    "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
    "\n",
    "    # normalise numbers by batch\n",
    "    for key, val in running_results.items():\n",
    "        running_results[key] /= num_batches\n",
    "\n",
    "    return running_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8DSgtnRmE-W"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, device, validation_dataloader, training_params : dict, metrics : dict):\n",
    "    \"\"\"\n",
    "    This method encapsulates the evaluation of a given model for one epoch.\n",
    "    \n",
    "    @param model : nn.Module                            Model to be trained.\n",
    "    @param device : str                                 Device to be trained on.\n",
    "    @param validation_dataloader : nn.data.DataLoader   DataLoader object to load batches of data.\n",
    "    @param training_params : dict                       Dictionary object mapping names of \n",
    "                                                        training utilities to their respective objects.\n",
    "                                                        Required are \"batch_size\", \"loss_function\", \n",
    "                                                        and \"optimizer\". \n",
    "    @param metrics : dict                               Dictionary object mapping names of \n",
    "                                                        metrics to a functional method that \n",
    "                                                        would compute the metric value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== INITIALIZE =====\n",
    "    # constants\n",
    "    BATCH_SIZE = training_params[\"batch_size\"]\n",
    "    LOSS_FUNCTION = training_params[\"loss_function\"]\n",
    "    OPTIMIZER = training_params[\"optimizer\"]\n",
    "\n",
    "    # to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # epoch statistics\n",
    "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
    "    running_results[\"loss\"] = 0.0\n",
    "\n",
    "    # ===== EVALUATE EPOCH =====\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        batches = 0\n",
    "        for index, batch in enumerate(validation_dataloader, 1):\n",
    "            \n",
    "            batches += 1\n",
    "\n",
    "            # input and ground truth\n",
    "            input_batched = batch[\"image\"]\n",
    "            ground_truth_batched = batch[\"reconstructed\"]\n",
    "            mask_batched = batch[\"mask\"]\n",
    "\n",
    "            # move tensors to device\n",
    "            input_batched = input_batched.to(device)\n",
    "            ground_truth_batched = ground_truth_batched.to(device)\n",
    "            mask_batched = mask_batched.to(device)\n",
    "\n",
    "            # reshape to channel first\n",
    "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
    "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
    "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
    "\n",
    "            # predict    \n",
    "            output_batched = model(input_batched)\n",
    "\n",
    "            # evaluate\n",
    "            loss = LOSS_FUNCTION(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
    "            running_results[\"loss\"] += loss\n",
    "            \n",
    "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====\n",
    "            for key, func in metrics.items():\n",
    "                running_results[key] += func(output_batched, ground_truth_batched).detach().item()\n",
    "\n",
    "            args = \"\"\n",
    "            for key, val in running_results.items():\n",
    "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
    "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
    "\n",
    "            # delete to ensure memory footprint\n",
    "            del loss\n",
    "            del input_batched\n",
    "            del output_batched\n",
    "\n",
    "    # normalise numbers by batch\n",
    "    for key, val in running_results.items():\n",
    "        running_results[key] /= batches\n",
    "\n",
    "    return running_results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5QdWxQDmGv8"
   },
   "outputs": [],
   "source": [
    "def train_evaluate(model, device, train_dataset, validation_dataset, training_params: dict, metrics: dict):\n",
    "\n",
    "    \"\"\"\n",
    "    This method encapsulates the training and evaluation loop of a given model.\n",
    "    \n",
    "    @param model : nn.Module                            Model to be trained.\n",
    "    @param device : str                                 Device to be trained on.\n",
    "    @param train_dataloader : nn.data.DataLoader        DataLoader object to load batches of data for training.\n",
    "    @param validation_dataloader : nn.data.DataLoader   DataLoader object to load batches of data for validation.\n",
    "    @param training_params : dict                       Dictionary object mapping names of \n",
    "                                                        training utilities to their respective objects.\n",
    "                                                        Required are \"num_epochs\", \"batch_size\", \"loss_function\", \n",
    "                                                        \"scheduler\", \"save_path\" and \"optimizer\". \n",
    "    @param metrics : dict                               Dictionary object mapping names of \n",
    "                                                        metrics to a functional method that \n",
    "                                                        would compute the metric value.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ===== INITIALIZE =====\n",
    "    # constants\n",
    "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
    "    BATCH_SIZE = training_params[\"batch_size\"]\n",
    "    SCHEDULER = training_params[\"scheduler\"]\n",
    "    SAVE_PATH = training_params[\"save_path\"]\n",
    "    NUM_WORKERS = 2\n",
    "    START_EPOCH = 0\n",
    "    SAMPLE_SIZE = 16\n",
    "    PLOT_EVERY = 1\n",
    "\n",
    "    # variables\n",
    "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
    "    train_results[\"loss\"] = []\n",
    "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
    "    eval_results[\"loss\"] = []\n",
    "\n",
    "    # dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # ===== TRAIN =====\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # train\n",
    "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
    "        num_batches = 0\n",
    "\n",
    "        # train every epoch\n",
    "        print(\"\\nTraining ...\")\n",
    "        results = train_epoch(model, device, train_dataloader, training_params, metrics)\n",
    "        for key, val in results.items():\n",
    "            train_results[key].append(val)\n",
    "\n",
    "        # evaluate every epoch\n",
    "        print(\"\\nEvaluating ...\")\n",
    "        results = evaluate_epoch(model, device, validation_dataloader, training_params, metrics)\n",
    "        for key, val in results.items():\n",
    "            eval_results[key].append(val)\n",
    "\n",
    "        # ===== EPOCH RESULTS =====\n",
    "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
    "\n",
    "        # ===== VISUALIZE =====\n",
    "        if epoch % PLOT_EVERY == 0:\n",
    "            print(\"plotting ...\")\n",
    "            loader = DataLoader(validation_dataset, batch_size = SAMPLE_SIZE, shuffle = True)\n",
    "            batch = next(iter(loader))\n",
    "\n",
    "            # predict and plot\n",
    "            output = model(batch[\"image\"].to(device).permute(0, 3, 1, 2)).detach().cpu().permute(0, 3, 1, 2)\n",
    "            fig, ax = plt.subplots(3, SAMPLE_SIZE, figsize = (SAMPLE_SIZE * 5, 15, ))\n",
    "            for i in range(SAMPLE_SIZE):\n",
    "                image = batch[\"image\"][i]\n",
    "                reconstructed = batch[\"reconstructed\"][i]\n",
    "                predicted = output[i]\n",
    "\n",
    "                if image.shape[-1] > 3: \n",
    "                    image = image[:, :, 0:3] # take rgb if more than 3 channels\n",
    "                    \n",
    "                ax[0][i].imshow(image)\n",
    "                ax[1][i].imshow(reconstructed)\n",
    "                ax[2][i].imshow(predicted)\n",
    "            plt.savefig(f\"{SAVE_PATH}_epoch{epoch + 1}.png\")\n",
    "            print(\"saved plots!\")\n",
    "            plt.close()\n",
    "\n",
    "        # ===== HOUSEKEEPING =====\n",
    "\n",
    "        # scheduler every epoch\n",
    "        if SCHEDULER is not None:\n",
    "            SCHEDULER.step(eval_results[\"loss\"][epoch])\n",
    "\n",
    "        # save save every epoch\n",
    "        SAVE = f\"{SAVE_PATH}_epoch{epoch + 1}.pt\"\n",
    "        torch.save(model.state_dict(), SAVE)\n",
    "        print(\"saved model!\")\n",
    "\n",
    "    return train_results, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBdz0CphnPYX"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGWU06i_Gznc",
    "outputId": "a243ec87-4663-4503-c93a-2c42d8041802"
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# train\n",
    "running_train_results, running_eval_results = train_evaluate(model, device, train_dataset, validation_dataset, training_params, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9PF_yqn578G"
   },
   "source": [
    "# Results and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pchFXMfmqytC"
   },
   "outputs": [],
   "source": [
    "def visualize_results(model, device, running_train_results: dict, running_eval_results: dict, test_dataset = None, images_only = False):\n",
    "\n",
    "    # get number of epochs and data points\n",
    "    NUM_EPOCHS = len(list(running_train_results.values())[0])\n",
    "    NUM_METRICS = len(list(running_train_results))\n",
    "    assert list(running_train_results.keys()) == list(running_eval_results.keys())\n",
    "    \n",
    "    if not images_only:\n",
    "        # plot \n",
    "        fig, ax = plt.subplots(NUM_METRICS, figsize = (NUM_EPOCHS * 5, NUM_METRICS * 5))\n",
    "        epochs_axis = [i for i in range(NUM_EPOCHS)]\n",
    "        index = 0\n",
    "        for key in list(running_train_results.keys()):\n",
    "            train = running_train_results[key]\n",
    "            validation = running_eval_results[key]\n",
    "\n",
    "            ax[index].plot(epochs_axis, train, label = \"train\")\n",
    "            ax[index].plot(epochs_axis, validation, label = \"validation\")\n",
    "            ax[index].title.set_text(key)\n",
    "            index += 1\n",
    "\n",
    "    if test_dataset != None:\n",
    "        SAMPLE_SIZE = 8\n",
    "        loader = DataLoader(test_dataset, batch_size = SAMPLE_SIZE, shuffle = True)\n",
    "        batch = next(iter(loader))\n",
    "\n",
    "        # predict\n",
    "        output = model(batch[\"image\"].to(device).permute(0, 3, 1, 1)).detach().cpu().view(0, 2, 3, 1)\n",
    "\n",
    "        fig, ax = plt.subplots(3, SAMPLE_SIZE, figsize = (SAMPLE_SIZE * 5, 15, ))\n",
    "        for i in range(SAMPLE_SIZE):\n",
    "            image = batch[\"image\"][i]\n",
    "            reconstructed = batch[\"reconstructed\"][i]\n",
    "            predicted = output[i]\n",
    "\n",
    "            if image.shape[-1] > 3: \n",
    "                image = image[:, :, 0:3] # take rgb if more than 3 channels\n",
    "                \n",
    "            ax[0][i].imshow(image)\n",
    "            ax[1][i].imshow(reconstructed)\n",
    "            ax[2][i].imshow(predicted)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "060KqhlANEnv"
   },
   "outputs": [],
   "source": [
    "visualize_results(model, device, running_train_results, running_eval_results, test_dataset = validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p52Y-_sF_LEl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wa4q3TAO5QWj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNet+partialConv",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
